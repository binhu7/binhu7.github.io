<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Bin Hu UIUC</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Bin Hu</div>
<div class="menu-item"><a href="index.html" class="current">home</a></div>
<div class="menu-item"><a href="research.html">Research</a></div>
  <div class="menu-item"><a href="people.html">People</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
</td>
<td id="layout-content">
  
  <div id="toptitle">
<h1>Bin Hu <br /></h1>
</div>
  
<table class="imgtable"><tr><td>
<img src="Figs/BinHu.jpg" alt="Bin Hu" width="121px" height="176px" />&nbsp;</td>

<td align="left"> Assistant Professor <br />
<a href="https://ece.illinois.edu/">Department of Electrical and Computer Engineering</a> <br />
<a href="https://csl.illinois.edu/">Coordinated Science Laboratory</a> <br />
<a href="https://www.illinois.edu">University of Illinois at Urbana-Champaign</a> </p>


<p>  <a href="mailto:binhu7@illinois.edu">binhu7@illinois.edu</a><br />
  145 CSL <br/>
1308 W Main St <br/>
Urbana, IL 61801 <br/></p>





</td></tr></table>
<h2>About Me</h2>
<p>I am an assistant professor in the Department of Electrical and Computer Engineering at 
the University of Illinois at Urbana-Champaign and affiliated with the Coordinated Science Laboratory. My research focuses on building fundamental connections between control and machine learning. Currently I am most interested in</p>
<ul>
<li><p> System/control tools for designing robust deep neural networks</p>
</li>
<li><p> Connections between robust control and reinforcement learning</p>
</li>
<li><p> Control-theoretic tools for analysis and design of iterative algorithms in optimization and learning</p>
</li>
</ul>
  
  
<p> I received the B.Sc. in Theoretical and Applied Mechanics from the University of Science and Technology of China in 2008, and received 
  the M.S. in Computational Mechanics from Carnegie Mellon University in 2010. I received the Ph.D. in Aerospace Engineering and Mechanics at 
  the University of Minnesota in 2016, under the supervision of Peter Seiler. Between July 2016 and July 2018, I was a postdoctoral researcher 
  in the Wisconsin Institute for Discovery at the University of Wisconsin-Madison. At Madison, I was working with Laurent Lessard and closely 
  collaborating with Stephen Wright. In 2021, I received the NSF CAREER award and the Amazon research award.</p>



<h2>News</h2>
<ul>
  <li><p><b>10/2022:</b> Our paper "Towards a Theoretical Foundation of Policy Optimization for Learning Control Policies" has been posted on <a href="https://arxiv.org/abs/2210.04810">arxiv</a>. This is an invited survey article which is going to appear in the next issue of <a href="https://www.annualreviews.org/journal/control">Annual Review of Control, Robotics, and Autonomous Systems</a>. </p>
  </li>
  
    <li><p><b>09/2022:</b> My student Xingang Guo and I managed to prove that direct policy search can be guaranteed to achieve global convergence on the H-infinity state-feedback synthesis problem! Our results are summarized in a new paper entitled "Global Convergence of Direct Policy Search for State-Feedback H-Infinity Robust Control: A Revisit of Nonsmooth Synthesis with Goldstein Subdifferential", which has been accepted to NeurIPS 2022 and posted on <a href="https://arxiv.org/abs/2210.11577">arxiv</a>. </p>
  </li>
  
  <li><p><b>05/2022:</b> Our paper "Provable Acceleration of Heavy Ball beyond Quadratics for a Class of Polyak-Lojasiewicz Functions when the Non-Convexity is Averaged-Out" has been accepted to <a href="https://icml.cc/Conferences/2022/Schedule?showEvent=16518">International Conference on Machine Learning (ICML) 2022</a>. </p>
  </li>
  
  <li><p><b>05/2022:</b> Our paper "Policy Optimization for Markovian Jump Linear Quadratic Control: Gradient Method and Global Convergence" has been accepted to <a href="https://ieeexplore.ieee.org/document/9779410">IEEE Transactions on Automatic Control</a>. </p>
  </li>
  
  <li><p><b>03/2022:</b> Our paper "Connectivity of the Feasible and Sublevel Sets of Dynamic Output Feedback Control with Robustness Constraints" has been posted <a href="https://arxiv.org/pdf/2203.11177.pdf">on arxiv</a>. This paper brings new insights for understanding policy optimization in the output feedback setting. </p>
     <p><b>Update:</b> The above paper has been accepted to <a href="https://ieeexplore.ieee.org/document/9814881">IEEE Control Systems Letters</a>.</p>
  </li> 
  
  <li><p><b>01/2022:</b> Our paper "Revisiting PGD Attacks for Stability Analysis of High-Dimensional Nonlinear Systems and Perception-Based Control" has been posted <a href="https://arxiv.org/pdf/2201.00801.pdf">on arxiv</a>. This paper tailors PGD attacks from the adversarial learning literature as scalable analysis tools for approximating the region of attraction (ROA) of nonlinear control systems with large-scale neural network policies and/or high-dimensional image observations.</p>
    <p><b>Update:</b> The above paper has been accepted to <a href="https://ieeexplore.ieee.org/document/9814867">IEEE Control Systems Letters</a>.</p>
  </li>
  
    <li><p><b>11/2021:</b> Our paper "Model-Free Î¼ Synthesis via Adversarial Reinforcement Learning" has been posted <a href="https://arxiv.org/pdf/2111.15537.pdf">on arxiv</a>. This paper builds a connection between adversarial reinforcement learning and the famous DK iteration algorithm from robust control.</p>
   <p><b>Update:</b> The above paper has been accepted to American Control Conference (ACC) 2022.</p>
  </li>
  
   <li><p><b>09/2021:</b> Our paper "Derivative-Free Policy Optimization for Linear Risk-Sensitive and Robust Control Design: Implicit Regularization and Sample Complexity" has been accepted to <a href="https://papers.nips.cc/paper/2021/hash/1714726c817af50457d810aae9d27a2e-Abstract.html">NeurIPS 2021</a>. </p>
  </li>
  
   <li><p><b>03/2021:</b> Delighted to receive the <a href="https://www.amazon.science/research-awards/program-updates/2020-amazon-research-awards-recipients-announced">2020 Amazon Research Award</a>. Big thanks to Amazon!</p>
  </li>
  
   <li><p><b>02/2021:</b> Thrilled to receive the <a href="https://nsf.gov/awardsearch/showAward?AWD_ID=2048168&HistoricalAwards=false">NSF CAREER award</a> on "Interplay between Control Theory and Machine Learning." Big thanks to NSF!</p>
  </li>
  
   <li><p><b>09/2020:</b> Our paper "On the Stability and Convergence of Robust Adversarial Reinforcement Learning: A Case Study on Linear Quadratic Systems" has been accepted to <a href="https://proceedings.neurips.cc/paper/2020/hash/fb2e203234df6dee15934e448ee88971-Abstract.html">NeurIPS 2020</a>. </p>
  </li>
  
     <li><p><b>03/2020:</b> Our paper "Analysis of Biased Stochastic Gradient Descent Using Sequential Semidefinite Programs" has been accepted to <a href="https://link.springer.com/article/10.1007%2Fs10107-020-01486-1">Mathematical Programming</a>. A full-text view-only version of the final paper can be found <a href="https://rdcu.be/b3b59">here</a>. </p>
  </li>
  
   <li><p><b>10/2019:</b> Our paper "Policy Optimization for H2 Linear Control with H-infinity Robustness Guarantee: Implicit Regularization and Global Convergence" has been posted <a href="https://arxiv.org/pdf/1910.09496.pdf">on arxiv</a>. This paper studies the implicit regularization mechanism in policy-based reinforcement learning for robust control design.</p>
  <p><b>Update I:</b> A conference version of the above paper has been accepted to <a href="https://sites.google.com/berkeley.edu/l4dc/home">L4DC 2020</a>. (one of 14/131 papers selected for oral presentation)</p>
    <p><b>Update II:</b> The journal version of the above paper has been accepted to SIAM Journal on Control and Optimization (SICON).</p>
  </li>
  
  <li><p><b>06/2019:</b> Our paper "Characterizing the Exact Behaviors of Temporal Difference Learning Algorithms Using Markov Jump Linear System Theory" has been posted <a href="https://arxiv.org/pdf/1906.06781.pdf">on arxiv</a>. This is my first paper on analyzing reinforcement learning algorithms using control theory!</p>
<p><b>Update:</b> The above paper has been accepted to <a href="https://nips.cc/Conferences/2019/Schedule?showEvent=13909">NeurIPS 2019</a>. The arxiv version of the paper has been revised.</p>
  </li>
  
<li><p><b>08/2018:</b> I started as an Assistant Professor in the Electrical and Computer Engineering Department at the University of Illinois at Urbana-Champaign.</p>
</li>
</ul>



</td>
</tr>
</table>
</body>
</html>
